\subsection{Systemarchitektur}

Das Materialflusssystem auf den \textsc{Mica}z-Modulen ist in einer Schichtenarchitektur aufgebaut. Diese ist an den Aufbau von AUTOSAR \cite{AUTOSAR:2014:Online} angelehnt. Abbildungen \ref{fig:architecture_ramp} und \ref{fig:architecture_vb} zeigen den Aufbau des Systems auf den Rampen beziehungsweise Volksbots.

\begin{figure}[h!]
 \centering
		\includegraphics[width=1\textwidth]{flow/Architektur_Rampe.png}
	\caption{Architektur der Rampe \cite{Stasch:Hahn}}
	\label{fig:architecture_ramp}
\end{figure}

\begin{figure}[h!]
 \centering
		\includegraphics[width=1\textwidth]{flow/Architektur_VB.png}
	\caption{Architektur der Volksbots aus Sicht des Materialfluss \cite{Stasch:Hahn}}
	\label{fig:architecture_vb}
\end{figure}

Ganz unten in der Hierarchie befindet sich die eigentliche \textsc{Mica}z Hardware (Hardware Level) mit allen Peripherie-Komponenten. Diese wird vom den darüber liegenden Background Level angesteuert. Im Backgrund Level befinden sich im je nach Modul hardwareabhängige Treiber für drahtlose und serielle Kommunikation, Lichtschranken, Bolzen und einen externen Flash-Speicher. Diese agieren meist auf Pin-Ebene, steuern also die einzelnen GPIOs des Mikrocontrollers. Eine Besonderheit stellt hier der Radio-Driver dar, der nicht direkt auf die Hardware, sondern auf den Kommunikationsstack des Echtzeitbetriebssystems Contiki zugreift.

Darüber finden sich Interfaces, die die Funktionen der Treiber aufbereiten und in Funktionen gliedern, die es den oberen Schichten erlauben, ohne großen Aufwand und Kenntnis der Implementierungsdetails (konkreter Ein- beziehungsweise Ausgangs-Pin, Timing, usw.) auf die Hardware zuzugreifen. Neben den Treibern und Interfaces befindet sich im Background Level auch das Echtzeitbetriebssystem Contiki OS. Dieses beinhaltet unter anderem einen Scheduler, eine Prozessverwaltung und den Kommunikationsstack \textit{Rime} (siehe \autoref{sec:rime}). 

Schließlich folgt auf der höchsten Hierarchieebene das Agent Level. Hier befindet sich zunächst das AgentRTE, eine Laufzeitumgebung für Agenten. Dieses ist weitgehend hardwareunabhängig. Lediglich bei der Prozessverwaltung gibt es noch Unterschiede die in \autoref{sec:AgentRTE} noch näher betrachtet werden.
Aufgaben des AgentRTE sind vor allem die Verwaltung aller Agenten auf der Plattform und deren Scheduling, sowie der Austausch von Nachrichten untereinander.

Das letzte Glied in der Kette bilden letztendlich die Agenten. Sie werden vom AgentenRTE verwaltet und sind grundsätzlich hardwareunabhängig. Die Agenten bilden die echte Betriebslogik des Systems ab und kommunizieren dafür untereinander mit Nachrichten. Auf jedem Modul gibt es einen Platform-, einen Order- und einen Routing-Agenten. Dazu können auf den Volksbots ein und auf den Rampen bis zu vier Paket-Agenten registriert sein.

In den folgenden Abschnitten wird nun die Implementierung des Materialflusssystems anhand dieser Architektur erläutert, beginnend beim Echtzeitbetriebssystem Contiki, über die Treiber und Interfaces hin zum AgentenRTE und schließlich den Agenten.

\subsubsection{Contiki}
Contiki ist ein quelloffenes Echtzeitbetriebssystem (RTOS: Real Time Operating System), das in dieser Projektgruppe auf den \textsc{Mica}z-Modulen eingesetzt wird \cite{Contiki:2014:Online}. Es ist speziell für die Anforderungen des Internet of Things und von Wireless Sensor Networks zugeschnitten und bietet einen einfachen ereignisgesteuerten Betriebssystemkern mit sogenannten Protothreads (Threads, die sich einen gemeinsamen Stack teilen und daher schnell gewechselt werden können), optionalem präemptives Multithreading, Interprozess-Kommunikation via Message-Passing mit Events, eine dynamische Prozessstruktur mit Unterstützung für das Laden und Beenden von Prozessen und einen nativen Kommunikationsstack für die drahtlose Kommunikation gemäß dem IEEE-Standard \textit{802.15.4} \cite{IEEE802154:2014:Online}.
 
\paragraph{Build-Vorgang}\mbox{}\\
Es existieren Implementierungen von und Treiber für Contiki für eine Vielzahl von Plattformen. Dazu gehören neben \textsc{Mica}z-Modulen auch etwa der \textit{MSP430x} von Texas Instruments oder der \textit{Atmega128 RFA1} von Atmel. Für welche Plattform ein Contiki-System und die darauf geplanten Anwendungen gebaut wird, wird zur Compile-Zeit entschieden. Das heißt, um die selbe Anwendung mit Contiki auf mehrere Plattformen zu bringen, muss die Anwendung für jede Zielplattform neu gebaut werden.

Für jede Plattform existiert dafür ein eigener Ordner im \textit{platforms}-Verzeichnis der Contiki-Quelldateien. Um nun das Zielsystem zu wählen, muss lediglich das \textit{TARGET} beim Aufruf des entsprechenden Makefiles angegeben werden und das Build-System inkludiert automatisch die passenden Treiber und Definitionen.

Projektdateien können über die Makefile-Variable \textit{PROJECT\_SOURCEFILES} hinzugefügt werden. \autoref{lst:contikimakefile} zeigt exemplarisch das Makefile der Rampen.

\lstinputlisting[language=C, style=customc, captionpos=b, caption={Makefile des Contiki-Systems der Rampen}, label=lst:contikimakefile]{src/flow/lst/makefile.lst}


\paragraph{Prozesse}\mbox{}\\
Prozesse in Contiki implementieren folgen einem Konzept namens Protothreads. Dies erlaubt es Prozessen, ohne den Speicher-Overhead und die langen 
Prozesswechselzeiten von normalen Threads auszukommen, indem sie sich einen gemeinsamen Stack auf dem Hauptspeicher teilen.
Einzige Einschränkungen dieser Entwicklung sind, dass in Prozessen keine Switch-Case-Anweisungen auftreten dürfen und dass nur statische und globale Variablen zwischen zwei Aufrufen erhalten bleiben. Dynamisch erzeugte Variablen werden dagegen überschrieben. Entsprechend sollte der Zustand eines Prozesses mithilfe von statischen Variablen gespeichert werden. \autoref{lst:process} zeigt eine solche statische Variable (i) und den vollständigen Aufbau eines Prozesses.

\lstinputlisting[language=C, style=customc, captionpos=b, caption={Einfacher Beispiel-Prozess in Contiki}, label=lst:process]{src/flow/lst/process_example.lst}

In Zeile 1 wird der Prozess initialisiert und in Zeile 2 automatisch beim Boot von Contiki gestartet. Zeile 4 beinhaltet die Deklaration. So können andere Prozesse diesem Prozess Events (mit oder ohne Daten) schicken, auf die unser Beispielprozess mit ev und data zugreifen kann. Zeile 6 kennzeichnet den Beginn der tatsächlichen Ablauflogik. Code über dieser Zeile wird bei jedem Prozessaufruf ausgeführt, dies wird jedoch in den meisten Fällen nicht benötigt. Zeile 13 schließlich beendet den Prozess und entfernt ihn aus der Prozess-Liste des Kernels. In diesem Beispiel wird die Zeile jedoch nie erreicht, sodass der Prozess immer wieder aufgerufen wird, bis er von einem anderen Prozess beendet wird.

\paragraph{Prozesskommunikation}\mbox{}\\
In Contiki kommunizieren Prozesse über Events. Auch der Kernel versendet Events, um Prozesse über ihren Zustand (Init, Continue, Exit) oder über abgelaufene Timer zu informieren. Zur Identifikation werden dabei Event IDs genutzt. Die Event IDs 0-127 können vom Benutzer frei vergeben werden, während die Prozess IDs ab 128 vom System genutzt werden. Grundsätzlich unterscheidet Contiki zwischen synchronen und asynchronen Events. 

\begin{itemize}
\item \textbf{Asynchrone Events} werden vom Kernel in einer Warteschlange gespeichert. Die Scheduling-Funktion des Kernels läuft nach Systemstart in einer Endlosschleife. In jedem Durchlauf wird ein Event aus der Schlange entnommen und an den Zielprozess weitergeleitet.
\item \textbf{Synchrone Events} gleichen einem Funktionsaufruf.
Sie werden ohne Umweg über die Warteschlange direkt an den Empfänger-Prozess
zugestellt \cite{Contiki:2014:Online}.  Mit der Funktion \textit{process\_post\_synch(\&example\_process, EVENT\_ID, msg)} wird gezielt ein Prozess aufgerufen (ein Broadcast ist nicht möglich). Während der aufgerufene Prozess aktiv ist, blockiert der Aufrufer und setzt seine Ausführung erst fort, wenn der aufgerufene Prozess die Kontrolle wieder abgibt.
\end{itemize}

Um auf Events zu reagieren, können in Prozessen die folgenden Funktionen genutzt werden:

\begin{itemize}
\item PROCESS\_WAIT\_EVENT() - Wartet auf ein beliebiges Event, bevor die Ausf\"{u}hrung fortgesetzt wird.
\item PROCESS\_WAIT\_EVENT\_UNTIL(condition) - Wartet auf ein beliebiges Event, setzt die Ausf\"{u}hrung aber nur fort, wenn die Bedingung erf\"{u}llt ist.
\item PROCESS\_WAIT\_UNTIL() - Wartet, bis die Bedingung erf\"ullt ist. Muss den Prozess nicht zwangsl\"{a}ufig anhalten.
\end{itemize}

Prozesse können neben Events auch über Polling-Anfragen kommunizieren. Polls werden bei der Bearbeitung von Hardware-Interrupts genutzt, da Interrupt-Handler keine Events absetzen dürfen. Sie können als Events mit erhöhter Priorität betrachtet werden. Ein Prozess, der einen Poll erhalten hat, wird in der Warteschlange für Prozesse priorisiert. \cite{Contiki:2014:Online, Walter:2010}.

\paragraph{Scheduling und Timer}\mbox{}\\
Grundsätzlich nutzt Contiki ein Event-getriebenes Modell von Nebenläufigkeit, wobei einzelne Events nach dem Run-To-Completion (RTC) Prinzip abgearbeitet werden. Das heißt, einmal angelaufen können Prozesse nur noch von Hardware-Interrupts unterbrochen werden oder selbst die Kontrolle abgeben. Dies ermöglicht es, alle Prozesse auf dem selben Stack arbeiten zu lassen und so Hauptspeicher zu sparen. Auf diese Weise muss kaum Speicher dynamisch alloziert werden. Außerdem werden so Race-Conditions auf geteilten Speicher nahezu ausgeschlossen. Dabei haben alle Prozesse und Events vorerst die gleiche Priorität und werden streng nacheinander in Reihenfolge abgearbeitet.

Es existiert eine Bibliothek die auch echte Threads mit jeweils einzelnen Stacks ermöglicht. Aufgrund des ohnehin knappen Hauptspeichers auf den \textsc{Mica}z-Modulen wurde diese Möglichkeit jedoch in der Projektgruppe nicht weiter betrachtet.

Ein Problem der Event-getrieben Nebenläufigkeit ist jedoch das Reaktionsvermögen auf Echtzeitanforderungen und externe Events: Sollte ein Prozess eine aufwändige Berechnung durchführen, kann es zu spät sein, bis er die Kontrolle abgibt. Aus diesem Grund führt Contiki eine zweite Prioritätsebene ein, sogenannte Polls. Diese werden zwischen asynchron auftretende Events geplant und rufen in Reihenfolge einer Priorität alle Prozesse auf, die ein Polling-Flag gesetzt haben. Üblicherweise sind dies insbesondere hardwarenahe Prozesse, die auf Änderungen an den Ein- und Ausgangspins beziehungsweise auf Timer reagieren müssen.
\paragraph{Der Rime Kommunikationsstack}\mbox{}\\
\label{sec:rime}
\input{src/flow/4_2_Treiber}
\input{src/flow/4_3_AgentRTE}
\input{src/flow/4_4_Agenten}
